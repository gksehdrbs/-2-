# -*- coding: utf-8 -*-
import sys
from pathlib import Path
import json
import numpy as np
import torch
from transformers import AutoTokenizer, AutoModel

# --------------------------------------------------------
# 0. ê²½ë¡œ ì„¤ì •
# --------------------------------------------------------
sys.path.append(str(Path(__file__).resolve().parents[1]))  # src
sys.path.append(str(Path(__file__).resolve().parents[2]))  # project

BASE_DIR = Path(__file__).resolve().parents[2]
PARSED_DIR = BASE_DIR / "parsed"
EMBED_DIR = BASE_DIR / "embeddings"
EMBED_DIR.mkdir(exist_ok=True)

MODEL_NAME = "BAAI/bge-m3"
tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)
model = AutoModel.from_pretrained(MODEL_NAME)

# --------------------------------------------------------
# 1. í…ìŠ¤íŠ¸ â†’ ì„ë² ë”© ë³€í™˜ í•¨ìˆ˜
# --------------------------------------------------------
def get_embedding(text: str):
    inputs = tokenizer(text, return_tensors="pt", truncation=True, padding=True)
    with torch.no_grad():
        embeddings = model(**inputs).last_hidden_state[:, 0, :]  # CLS í† í°
    return embeddings[0].cpu().numpy().astype("float32")

# --------------------------------------------------------
# 2. ì„ë² ë”© ì‹¤í–‰ í•¨ìˆ˜ (main.pyì—ì„œ í˜¸ì¶œ ê°€ëŠ¥)
# --------------------------------------------------------
def run_embedding(input_path: Path = None):
    """
    íŒŒì‹±ëœ JSON íŒŒì¼ì„ ë¶ˆëŸ¬ì™€ ì„ë² ë”©ì„ ìˆ˜í–‰í•˜ê³ 
    .npy / .json íŒŒì¼ë¡œ ì €ì¥
    """
    if input_path is None:
        input_path = PARSED_DIR / "parsed_docx.json"

    if not input_path.exists():
        raise FileNotFoundError(f"âŒ íŒŒì‹±ëœ JSON íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {input_path}")

    with open(input_path, "r", encoding="utf-8") as f:
        data = json.load(f)

    if isinstance(data, list):
        documents = data
    elif isinstance(data, dict):
        documents = data.get("documents", [])
    else:
        raise ValueError("ì§€ì›í•˜ì§€ ì•ŠëŠ” JSON êµ¬ì¡°ì…ë‹ˆë‹¤.")

    if not documents:
        raise ValueError("âŒ ë¬¸ì„œ ë‚´ìš©ì´ ë¹„ì–´ ìˆìŠµë‹ˆë‹¤.")

    print(f"ğŸ“˜ ë¬¸ì¥ ê°œìˆ˜: {len(documents)}ê°œ â†’ ì„ë² ë”© ìƒì„± ì¤‘...")
    vectors = np.array([get_embedding(doc) for doc in documents])
    print("âœ… ì„ë² ë”© shape:", vectors.shape)

    np.save(EMBED_DIR / "doc_embeddings.npy", vectors)

    embedding_data = [
        {"text": doc, "vector": vec.tolist()}
        for doc, vec in zip(documents, vectors)
    ]
    with open(EMBED_DIR / "doc_embeddings.json", "w", encoding="utf-8") as f:
        json.dump(embedding_data, f, ensure_ascii=False, indent=2)

    print(f"âœ… ì„ë² ë”© ì €ì¥ ì™„ë£Œ â†’ {EMBED_DIR / 'docx_embeddings.npy'}")
    print(f"âœ… í…ìŠ¤íŠ¸ ë§¤í•‘ ì €ì¥ ì™„ë£Œ â†’ {EMBED_DIR / 'docx_embeddings.json'}")

    return vectors


# --------------------------------------------------------
# 3. ë‹¨ë… ì‹¤í–‰ ê°€ëŠ¥í•˜ë„ë¡ ìœ ì§€
# --------------------------------------------------------
if __name__ == "__main__":
    run_embedding()
